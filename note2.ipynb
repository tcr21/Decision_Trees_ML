{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095\n",
      "[[ 0  4  1  5  3 21]\n",
      " [ 9  1  0  1  1 25]\n",
      " [ 2  5  1  4  0 14]\n",
      " [ 3  5  0  2  2 15]\n",
      " [ 3 26  0  1  1  3]\n",
      " [ 6 16  0  2  4 14]]\n",
      "precison: [0.         0.01754386 0.5        0.13333333 0.09090909 0.15217391]\n",
      "macro prec: 0.14899336615583755\n",
      "recall [0.         0.02702703 0.03846154 0.07407407 0.02941176 0.33333333]\n",
      "macro recall: 0.08371795626697587\n",
      "f1: [0.         0.0212766  0.07142857 0.0952381  0.04444444 0.20895522]\n",
      "macro f1: 0.07355715512273149\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from numpy.random import default_rng\n",
    "import numpy as np\n",
    "from classification import (\n",
    "    get_entropy,\n",
    "    get_information_gain,\n",
    "    find_best_node,\n",
    "    induce_decision_tree,\n",
    "    split_dataset,\n",
    "    Decision_Node,\n",
    "    Leaf_Node,\n",
    ")\n",
    "\n",
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \"\"\" Compute the confusion matrix.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "        class_labels (np.ndarray): a list of unique class labels. \n",
    "                               Defaults to the union of y_gold and y_prediction.\n",
    "\n",
    "    Returns:\n",
    "        np.array : shape (C, C), where C is the number of classes. \n",
    "                   Rows are ground truth per class, columns are predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # if no class_labels are given, we obtain the set of unique class labels from\n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n",
    "\n",
    "    # for each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        # get predictions where the ground truth is the current class label\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # quick way to get the counts per label\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # convert the counts to a dictionary\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # fill up the confusion matrix for the current row\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n",
    "\n",
    "\n",
    "def precision(y_gold, y_prediction):\n",
    "    \"\"\" Compute the precision score per class given the ground truth and predictions\n",
    "        \n",
    "    Also return the macro-averaged precision across classes.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple (precisions, macro_precision) where\n",
    "            - precisions is a np.ndarray of shape (C,), where each element is the \n",
    "              precision for class c\n",
    "            - macro-precision is macro-averaged precision (a float) \n",
    "    \"\"\"\n",
    "\n",
    "    confusion = confusion_matrix(y_gold, y_prediction)\n",
    "    p = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[:, c]) > 0:\n",
    "            p[c] = confusion[c, c] / np.sum(confusion[:, c])\n",
    "\n",
    "    ## Alternative solution without computing the confusion matrix\n",
    "    #class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "    #p = np.zeros((len(class_labels), ))\n",
    "    #for (c, label) in enumerate(class_labels):\n",
    "    #    indices = (y_prediction == label) # get instances predicted as label\n",
    "    #    correct = np.sum(y_gold[indices] == y_prediction[indices]) # intersection\n",
    "    #    if np.sum(indices) > 0:\n",
    "    #        p[c] = correct / np.sum(indices)     \n",
    "\n",
    "    # Compute the macro-averaged precision\n",
    "    macro_p = 0.\n",
    "    if len(p) > 0:\n",
    "        macro_p = np.mean(p)\n",
    "    \n",
    "    return (p, macro_p)\n",
    "\n",
    "\n",
    "def recall(y_gold, y_prediction):\n",
    "    \"\"\" Compute the recall score per class given the ground truth and predictions\n",
    "        \n",
    "    Also return the macro-averaged recall across classes.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple (recalls, macro_recall) where\n",
    "            - recalls is a np.ndarray of shape (C,), where each element is the \n",
    "                recall for class c\n",
    "            - macro-recall is macro-averaged recall (a float) \n",
    "    \"\"\"\n",
    "\n",
    "    confusion = confusion_matrix(y_gold, y_prediction)\n",
    "    r = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[c, :]) > 0:\n",
    "            r[c] = confusion[c, c] / np.sum(confusion[c, :])\n",
    "\n",
    "    ## Alternative solution without computing the confusion matrix\n",
    "    #class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "    #r = np.zeros((len(class_labels), ))\n",
    "    #for (c, label) in enumerate(class_labels):\n",
    "    #    indices = (y_gold == label) # get instances for current label\n",
    "    #    correct = np.sum(y_gold[indices] == y_prediction[indices]) # intersection\n",
    "    #    if np.sum(indices) > 0:\n",
    "    #        r[c] = correct / np.sum(indices)     \n",
    "\n",
    "    # Compute the macro-averaged recall\n",
    "    macro_r = 0.\n",
    "    if len(r) > 0:\n",
    "        macro_r = np.mean(r)\n",
    "    \n",
    "    return (r, macro_r)\n",
    "\n",
    "\n",
    "def f1_score(y_gold, y_prediction):\n",
    "    \"\"\" Compute the F1-score per class given the ground truth and predictions\n",
    "        \n",
    "    Also return the macro-averaged F1-score across classes.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple (f1s, macro_f1) where\n",
    "            - f1s is a np.ndarray of shape (C,), where each element is the \n",
    "              f1-score for class c\n",
    "            - macro-f1 is macro-averaged f1-score (a float) \n",
    "    \"\"\"\n",
    "\n",
    "    (precisions, macro_p) = precision(y_gold, y_prediction)\n",
    "    (recalls, macro_r) = recall(y_gold, y_prediction)\n",
    "\n",
    "    # just to make sure they are of the same length\n",
    "    assert len(precisions) == len(recalls)\n",
    "\n",
    "    f = np.zeros((len(precisions), ))\n",
    "    for c, (p, r) in enumerate(zip(precisions, recalls)):\n",
    "        if p + r > 0:\n",
    "            f[c] = 2 * p * r / (p + r)\n",
    "\n",
    "    # Compute the macro-averaged F1\n",
    "    macro_f = 0.\n",
    "    if len(f) > 0:\n",
    "        macro_f = np.mean(f)\n",
    "    \n",
    "    return (f, macro_f)\n",
    "\n",
    "\n",
    "def accuracy(y_gold, y_prediction):\n",
    "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
    "\n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float : the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_gold) == len(y_prediction)  \n",
    "    \n",
    "    try:\n",
    "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
    "    except ZeroDivisionError:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def read_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Read .txt file from a specified filepath. (i.e. data/train_full.txt)\n",
    "    Returns 2 numpy arrays of the instances and the labels\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(filepath, dtype=str, delimiter=\",\")\n",
    "    instances = data[:,:-1]\n",
    "    instances = instances.astype(int)\n",
    "    labels = data[:,-1]\n",
    "\n",
    "    return instances, labels\n",
    "\n",
    "\n",
    "def induce_random_decision_tree(instances, labels, num_attributes_per_tree, depth=0):\n",
    "    \"\"\"Induce a random decision tree for a given dataset\n",
    "\n",
    "    Args:\n",
    "    instances (numpy.ndarray): Instances, numpy array of shape (N, K)\n",
    "                       N is the number of instances\n",
    "                       K is the number of attributes\n",
    "    labels (numpy.ndarray): Class labels, numpy array of shape (N, )\n",
    "                       Each element in labels is a str\n",
    "    num_attributes_per_tree (int): Number of attributes to be used in each tree\n",
    "    Returns:\n",
    "      DecisionNode: A decision node object which contains the attribute, value, left node, right node and depth\n",
    "    \"\"\"\n",
    "    # Get a shuffled array of the indexes of the attributes\n",
    "    random_indexes = default_rng().permutation(len(instances[0, :]))\n",
    "    # Take the first attributes_per_tree indexes\n",
    "    random_indexes_in_each_tree = random_indexes[:num_attributes_per_tree]\n",
    "    # Create a new np array with only the attributes we want\n",
    "    new_instances = instances[:, random_indexes_in_each_tree]\n",
    "\n",
    "    # Find the best split point for the data\n",
    "    best_attr, best_split = find_best_node(new_instances, labels)\n",
    "\n",
    "    sorted_indices = instances[\n",
    "        :, best_attr\n",
    "    ].argsort()  # returns sorted instance indices based on the best attribute\n",
    "    sorted_instances = instances[sorted_indices]  # sort the instances\n",
    "    sorted_labels = labels[sorted_indices]  # sort the labels\n",
    "\n",
    "    # Split the total dataset based on the best split point\n",
    "    (\n",
    "        split_instances_left,\n",
    "        split_instances_right,\n",
    "        split_labels_left,\n",
    "        split_labels_right,\n",
    "    ) = split_dataset(sorted_instances, sorted_labels, best_split)\n",
    "\n",
    "    # If we can no longer split data, return a leaf node\n",
    "    if (\n",
    "        len(set(labels)) == 1\n",
    "        or len(split_instances_left) == 0\n",
    "        or len(split_instances_right) == 0\n",
    "    ):\n",
    "        return Leaf_Node(labels, depth)\n",
    "\n",
    "    # Otherwise return a decision node\n",
    "    LeftNode = induce_random_decision_tree(\n",
    "        split_instances_left,\n",
    "        split_labels_left,\n",
    "        num_attributes_per_tree,\n",
    "        depth + 1,\n",
    "    )\n",
    "    RightNode = induce_random_decision_tree(\n",
    "        split_instances_right,\n",
    "        split_labels_right,\n",
    "        num_attributes_per_tree,\n",
    "        depth + 1,\n",
    "    )\n",
    "\n",
    "    # Remap the best attribute its original index\n",
    "    best_attr = random_indexes_in_each_tree[best_attr]\n",
    "\n",
    "    # what to return if left node or right node does not exist\n",
    "    return Decision_Node(\n",
    "        best_attr, sorted_instances[best_split][best_attr], LeftNode, RightNode, depth\n",
    "    )\n",
    "\n",
    "decision_trees = []\n",
    "number_of_trees = 2\n",
    "\n",
    "#Read in data\n",
    "train_instances, train_labels = read_dataset(\"data/train_full.txt\")\n",
    "test_instances, test_labels = read_dataset(\"data/test.txt\")\n",
    "\n",
    "#Get number of attributes in the dataset\n",
    "num_attributes = len(train_instances[0, :])\n",
    "#Take the square root of number of attributes and round up (optimal number of attibutes per tree?)\n",
    "attributes_per_tree = math.ceil(np.sqrt(num_attributes))\n",
    "\n",
    "\n",
    "# Loop through number of trees and train each tree\n",
    "for i in range(0, number_of_trees):\n",
    "    # Create a new random combination of the row indexes from new_instances\n",
    "    random_row_indexes = default_rng().choice(len(train_instances), len(train_instances))\n",
    "    new_instances = train_instances[random_row_indexes, :]\n",
    "    new_labels = train_labels[random_row_indexes]\n",
    "\n",
    "    # Now we have the new instances and labels, we can create a new decision tree\n",
    "    new_decision_tree = induce_random_decision_tree(\n",
    "        new_instances, new_labels, attributes_per_tree\n",
    "    )\n",
    "    decision_trees.append(new_decision_tree)\n",
    "\n",
    "    \n",
    "predictions1 = []\n",
    "for i in test_instances:\n",
    "    votes = []\n",
    "    for j in range(len(decision_trees)):\n",
    "        votes.append(decision_trees[j].predict(i))\n",
    "    most_voted_label = max(set(votes), key=votes.count)\n",
    "    predictions1.append(most_voted_label)\n",
    "\n",
    "\n",
    "print(accuracy(test_labels, predictions1))\n",
    "confusion = confusion_matrix(test_labels, np.array(predictions1))\n",
    "print(confusion)\n",
    "(p, macro_p) = precision(test_labels, np.array(predictions1))\n",
    "print(\"precison:\", p)\n",
    "print(\"macro prec:\", macro_p)\n",
    "(r, macro_r) = recall(test_labels, np.array(predictions1))\n",
    "print(\"recall\", r)\n",
    "print(\"macro recall:\", macro_r)\n",
    "(f1, macro_f1) = f1_score(test_labels, np.array(predictions1))\n",
    "print(\"f1:\", f1)\n",
    "print(\"macro f1:\", macro_f1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba98291211580fa028395cdbed8301077dbc9954268562ee48d534efb29f1f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
